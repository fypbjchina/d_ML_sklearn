{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5285ca0d-7ef4-4d44-befa-1a2209cf689a",
   "metadata": {},
   "source": [
    "# 百页机器学习入门书 / 20240912"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6de369-fd09-43e7-8a03-c1c76cb87147",
   "metadata": {},
   "source": [
    "！！！🐍 由于原著版权原因，此学习笔记仅限个人交流学习目的，不做任何商业或广播用途。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00a4b7-7a43-4fb7-b677-f4201f3f1cb2",
   "metadata": {},
   "source": [
    "🐍 This book is distributed on the \"read first, buy later\" principle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606f066-2836-4cd4-a81f-facc8c9e9c0f",
   "metadata": {},
   "source": [
    "🐍 Reference: Machine Learning in Python --> https://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fae89-186a-4104-979e-8f12358af553",
   "metadata": {},
   "source": [
    "# 前 言"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b525ac34-6a11-4c9a-977e-209d82af7ade",
   "metadata": {},
   "source": [
    "Let's start by telling the truth: machines don't learn.\n",
    "What a typical \"learning machine\" does, is finding a mathematical formula, which, when applied to a collection of inputs (called \"training data\"), produces the desired outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780f7b7-a396-4537-9df6-8c4e9c874b5d",
   "metadata": {},
   "source": [
    "# 目 录\r\n",
    "\r\n",
    "1. **简介**\r\n",
    "   - 介绍了机器学习的定义及其实际应用，并讨论了监督学习、非监督学习、半监督学习、强化学习的不同类型。\r\n",
    "\r\n",
    "2. **基本定义与符号**\r\n",
    "   - 定义了书中将使用的符号、数据结构、集合运算、向量运算等，并介绍了参数估计、贝叶斯规则、分类与回归等基础概念。\r\n",
    "\r\n",
    "3. **核心算法**\r\n",
    "   - 详细讲解了线性回归、逻辑回归、决策树、支持向量机（SVM）等经典机器学习算法的定义、问题与解决方案。\r\n",
    "\r\n",
    "4. **算法工作原理**\r\n",
    "   - 介绍了学习算法的构建模块，包括梯度下降、模型训练方式，以及机器学习工程师的工作流程。\r\n",
    "\r\n",
    "5. **实践指南**\r\n",
    "   - 涵盖了特征工程（如独热编码、归一化处理）、正则化方法、模型评估、混淆矩阵、精确率与召回率、超参数调整等实践内容。\r\n",
    "\r\n",
    "6. **神经网络与深度学习**\r\n",
    "   - 介绍了神经网络的基本架构、多层感知器、卷积神经网络（CNN）、循环神经网络（RNN）等深度学习模型。\r\n",
    "\r\n",
    "7. **常见问题与解决方案**\r\n",
    "   - 针对分类问题、集成学习（如随机森林、提升方法）、序列标注、半监督学习、迁移学习等提出了具体的算法解决思路。\r\n",
    "\r\n",
    "8. **高级实践**\r\n",
    "   - 讨论了不平衡数据集处理、模型组合、神经网络训练、正则化技巧、迁移学习等高级技术。\r\n",
    "\r\n",
    "9. **无监督学习**\r\n",
    "   - 涵盖了密度估计、聚类分析（如K-Means、DBSCAN）、降维方法（PCA、UMAP）以及异常检测等技术。\r\n",
    "\r\n",
    "10. **其他学习形式**\r\n",
    "    - 提及了度量学习、排序学习、推荐系统、因子分解机等，并重点讨论了自监督学习（如词向量）。\r\n",
    "\r\n",
    "11. **结论与未涉及的内容**\r\n",
    "    - 讨论了一些未覆盖的主题，包括主题模型、生成对抗网络（GAN）、遗传算法、马尔可夫链蒙特卡洛（MCMC）、强化学习等。\r\n",
    "\r\n",
    "总的来说，这本书系统且简洁地介绍了机器学习的核心概念和实践技巧，是为机器学习从业者提供的全面指南【6:16†source】。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a55fe8-db8e-4552-a866-f8a7e790297c",
   "metadata": {},
   "source": [
    "# 第1章 简介\n",
    "\n",
    "## 1.1 什么是机器学习？\n",
    "机器学习是计算机科学的一个分支，旨在构建依赖于某些现象的示例数据的算法。这些示例可以来自自然界、人类手工制作或其他算法生成的。机器学习的目标是通过数据集，利用统计模型解决实际问题。\n",
    "\n",
    "## 1.2 学习类型\n",
    "- **监督学习**：数据集由带标签的示例组成，算法通过这些示例学习，生成一个模型用来预测新数据的标签。\n",
    "- **无监督学习**：数据集不包含标签，算法通过分析数据的特征来寻找模式，如聚类和降维。\n",
    "- **半监督学习**：数据集既有带标签的示例，也有未标记的示例，通常未标记的示例数量远超过带标签的示例，目的是通过未标记数据提升模型性能。\n",
    "- **强化学习**：算法在一个环境中不断执行动作，基于反馈（奖励或惩罚）学习最优决策策略。此类算法常用于解决序列决策问题，如游戏和机器人控制。\n",
    "\n",
    "## 1.3 监督学习如何工作\n",
    "监督学习的过程包括以下步骤：\n",
    "1. **数据收集**：收集一组输入输出配对数据，输入可以是电子邮件、图片等，输出可以是标签（如“垃圾邮件”或“非垃圾邮件”）或实数（如房价）。\n",
    "2. **特征转换**：将原始数据转换为机器可读的特征向量。例如，将文本转换为“词袋模型”（Bag of Words）。\n",
    "3. **模型训练**：使用算法（如支持向量机SVM）基于特征向量训练模型，寻找最佳决策边界以区分不同类别。\n",
    "4. **优化目标**：通过优化算法找到最优的模型参数，使模型能够最大限度地正确预测新数据的标签。\n",
    "\n",
    "## 1.4 为什么模型能在新数据上有效工作？\n",
    "机器学习模型可以在新数据上做出正确预测，因为它基于从训练数据中学到的决策边界。在合理假设下，新数据与训练数据在特征空间中的分布相似，因此模型可以有效地将不同类别分开。随着训练数据集的增大，模型对新数据的预测精度也会提高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c59628-aefa-4051-95bd-47dc725390dd",
   "metadata": {},
   "source": [
    "# 第2章 符号和定义\n",
    "\n",
    "## 2.1 符号\n",
    "- **标量**：简单的数值（如15或-3.25），用斜体字母表示（如x或a）。\n",
    "- **向量**：有序的标量列表，通常用粗体字母表示（如x或w），可视为多维空间中的点或箭头。\n",
    "- **矩阵**：由数值按行列排列的数组，通常用粗体大写字母表示（如A或W）。矩阵可以与向量相乘。\n",
    "- **集合**：无序的唯一元素集合，表示为书法体大写字母（如S）。常用交集（∩）和并集（∪）等运算符#。\n",
    "\n",
    "### 2.1.2 大写Sigma符号\n",
    "表示对集合或向量的元素进行求和，符号为 ∑，常用于定义和累加多个变#量的值。\n",
    "\n",
    "### 2.1.3 大写Pi符号\n",
    "表示乘积运算，符号为 ∏，类似于Sigma符号，但表示的是所#有元素的乘积。\n",
    "\n",
    "### 2.1.4 集合的操作\r\n",
    ". **派生集合创建操作**：用于创建一个新的集合。例如，`S' ← {x² | x ∈ S, x > 3}` 表示创建一个新集合 \\(S'\\)，其中包括所有 \\(x\\) 属于集合 \\(S\\) 且 \\(x > 3\\) 的元素的方。--2. **基数运算符**：运算符 `|S|` 用于返回集合 \\(S\\) 中的元素数量，即集合的  数计\n",
    "算。\n",
    "### 2.1.5 向量运算\n",
    "向量可以进行加减法、标量乘法和点积运算。点积运算的结果是标量，矩阵与向量相乘#的结果是另一个向量。\n",
    "\n",
    "### 2.1.6 函数\n",
    "函数表示输入到输出的映射。局部最小值表示函数在某个点附近的值是最小的，全球最小值是#所有局部最小值中的最小##\n",
    "\n",
    "###*2.1.7 M and rg Max*  \r\n",
    "- `max` 和 `arg max` 是常用的数学运算符。  \r\n",
    "- `maxa∈A f(a)`：在集合A中找到使得 f(a) 取得最大值的a。  \r\n",
    "- `arg maxa∈A f(a)`：找到使得 f(a) 最大的元素a。  \r\n",
    "- 当集合是隐含或无限时，可以简化为 `maxa f(a)` 或 `arg maxa f(a)`。\r\n",
    "- 类似的操作符还有 `min` 和 `arg min`，它们用于\n",
    "## # 2.1.8.e】。\r\n",
    "值运算符**  \r\n",
    "- 赋值运算符`←`用于为变量赋值。  \r\n",
    "- 表达式 `a ← f(x)` 表示变量 `a` 获得了 `f(x)` 的新值。  \r\n",
    "- 例如，`a ← [a1, a2]` 表示 `a` \n",
    "\n",
    "  1†source】。\n",
    "### 2.1.9 导数和梯度\n",
    "- **导数率描述函数的变化处，表示函数在某- - *的增长或减少广导数在*梯度**：广广是多维空间中的推##，表函数对多个输入变量的变化率。\n",
    "\n",
    "## 2.2 随机变量\n",
    "- **离散随机变量**：取有限个值，用概率质量函数（pmf）描述。\n",
    "- **连续随机变量**：取无限多个值，用概率密度函数（pdf）描述。\n",
    "\n",
    "期望值 \\(E[X]\\) 表示随机变量的平均值，是最重要##统计之一，标准差和方差则衡量数据的离散程度。\n",
    "\n",
    "## 2.3 无偏估计\n",
    "无偏估计量是对未知统计量的估计##其期值等于真实统计量。样本均值是期望值的无偏估计。\n",
    "\n",
    "## 2.4 贝叶斯定理\n",
    "贝叶斯定理用于计算条件概率，公式如下：\n",
    "\\[\n",
    "Pr(X = x|Y = y) = \\frac{Pr(Y = y|X = x## cdot Pr(X = x)}{Pr(Y = y)}\n",
    "\\]\n",
    "\n",
    "## 2.5 参数估计\n",
    "通过贝叶斯定理更新型##参数高斯分布模型是常见的概率分布模型，通过贝叶斯定理可以估计其参数。\n",
    "\n",
    "## 2.6 参数与超参数\n",
    "- **参数**：模型中的变量，由学习算法根据训练数据自动调整。\n",
    "##- *超参数**：影响算法工作方式的参数，通常由数据分析员在算法运行前设置。\n",
    "\n",
    "## 2.7 分类与回归\n",
    "- **分类**：自动将标签分配给未标记的数据##例，见的例子是垃圾邮件检测。\n",
    "- **回归**：预测连续的实数值标签，如房价预测。\n",
    "\n",
    "## 2.8 基于模型与基于实例的学习\n",
    "- **基于模型**：通过训练数据##建模（如SVM）。\n",
    "- **基于实例**：使用整个数据集作为模型，如k近邻算法（kNN）。\n",
    "\n",
    "## 2.9 浅层学习与深度学习\n",
    "- **浅层学习**：直接从特征中学习模型参数。\n",
    "- **深度学习**：通过多层神经网络学习，参数从每一层的输出中逐步调整。\n",
    "\n",
    "总结起来，这一章介绍了机器学习的基本符号、数据结构、向量和矩阵运算等，建立了概率论、统计学及机器学习中的分类与回归等基础知识，为后续更深入的算法学习打下基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03bfc7-f2b1-4e17-99ca-1f8b15fd0add",
   "metadata": {},
   "source": [
    "# 第3章 核心算法\n",
    "\n",
    "## 3.1 线性回归（Linear Regression）\n",
    "**线性回归**是用于回归任务的学习算法，目标是学习一个输入特征的线性组合模型：\n",
    "- **问题陈述**：给定一组带标签的数据，线性回归模型希望找到最优的参数`w`和`b`，使得模型对新输入的预测值与真实值尽可能接近。\n",
    "- **解决方案**：通过最小化均方误差（MSE）找到最优参数，MSE定义为所有样本预测误差平方的平均值。\n",
    "\n",
    "## 3.2 逻辑回归（Logistic Regression）\n",
    "尽管名为“回归”，**逻辑回归**实际上是一个分类算法，通常用于**二分类问题**。\n",
    "- **问题陈述**：将标签`y`视为二进制值（如0或1），并通过特征线性组合后的**逻辑函数**（Sigmoid函数）预测类别概率。\n",
    "- **解决方案**：通过最大化数据集的**似然函数**找到最优参数，使用梯度下降法优化。\n",
    "\n",
    "## 3.3 决策树学习（Decision Tree Learning）\n",
    "**决策树**是一种用于分类和回归的树状结构模型。\n",
    "- **问题陈述**：目标是从数据中学习一个决策树，以便通过递归分裂特征空间来预测类别。\n",
    "- **解决方案**：决策树算法如ID3根据特征和阈值递归划分数据集，通过最小化**熵**或最大化信息增益来评估划分效果。\n",
    "\n",
    "## 3.4 支持向量机（Support Vector Machine, SVM）\n",
    "**支持向量机**是一种常用于分类任务的算法。\n",
    "- **问题陈述**：SVM通过寻找最佳的**超平面**来分隔不同类别的数据点。若数据线性不可分，则通过**核技巧**将数据映射到高维空间。\n",
    "- **解决方案**：通过最小化模型的规范化误差（如**合页损失函数**）来找到最大间隔的超平面，并使用Lagrange乘子法优化问题。\n",
    "\n",
    "## 3.5 k近邻算法（k-Nearest Neighbors, kNN）\n",
    "**k近邻算法**是一种**基于实例**的学习算法。\n",
    "- **问题陈述**：给定一个未标记的数据点，kNN通过找到其最近的k个已标记点来预测类别。\n",
    "- **解决方案**：通过计算特征空间中的距离来识别最近邻居，并通过多数投票决定类别。\n",
    "\n",
    "## 主要总结\n",
    "- **线性回归**用于回归任务，模型简单且易于训练，但可能对非线性数据表现较差。\n",
    "- **逻辑回归**虽然也基于线性模型，但引入了逻辑函数，适用于分类问题。\n",
    "- **决策树**提供了直观的决策流程，通过分裂特征空间进行分类。\n",
    "- **SVM**通过最大化分类边界的间隔，确保模型的泛化能力，且可使用核函数处理非线性问题。\n",
    "- **kNN**是一种无需训练的算法，通过实例进行分类，但在大数据集上计算复杂度较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2534af7-e0b5-4d2c-911d-6d6810191a1b",
   "metadata": {},
   "source": [
    "# 第4章 学习算法的解剖\n",
    "\n",
    "本章介绍了机器学习算法的核心组成部分，以及如何使用它们来实现有效的模型训练。\n",
    "\n",
    "## 4.1 学习算法的基本构件\n",
    "一个学习算法主要包含三个部分：\n",
    "1. **损失函数（Loss Function）**：用于评估模型的预测与实际结果之间的差异。\n",
    "2. **优化标准（Optimization Criterion）**：通过损失函数确定优化目标，通常通过代价函数来表示。\n",
    "3. **优化例程（Optimization Routine）**：通过使用训练数据调整模型的参数，以找到最优解。\n",
    "\n",
    "一些算法显式地优化某个特定标准，例如线性回归和逻辑回归，而其他算法（如决策树和K近邻）则隐式地进行优化。\n",
    "\n",
    "## 4.2 梯度下降法（Gradient Descent）\n",
    "梯度下降法是一种常见的迭代优化算法，主要用于找到函数的局部最小值。算法通过计算函数梯度的负方向来更新参数，以逐步逼近最小值。该方法适用于线性回归、逻辑回归和神经网络等模型。\n",
    "\n",
    "- **梯度下降法的步骤**：从随机点开始，根据梯度的方向进行小步调整，直到找到局部最小值。\n",
    "- **凸优化问题**：对于逻辑回归和支持向量机等凸优化问题，梯度下降可以确保找到全局最小值。\n",
    "- **非凸优化问题**：对于神经网络等非凸优化问题，即便找到的是局部最小值，也能在实践中产生有效的结果。\n",
    "\n",
    "## 4.3 机器学习工程师的工作方式\n",
    "机器学习工程师通常不会自己实现复杂的算法，而是借助现有的开源库，如scikit-learn。通过调用库中的函数，可以轻松地实现诸如线性回归、逻辑回归、决策树等常用算法。\n",
    "\n",
    "## 4.4 学习算法的特殊性\n",
    "不同的学习算法具有不同的超参数和特性。例如，SVM和逻辑回归期望输入为数值特征，而决策树可以处理类别特征。此外，某些算法支持增量学习，即可以在获得新的训练数据后继续更新模型，而不需要从头开始训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12064a8-cbb9-4abe-8814-e6a507381544",
   "metadata": {},
   "source": [
    "# 第5章 基础实践\n",
    "\n",
    "## 5.1 特征工程\n",
    "特征工程是将原始数据转换为数据集的过程。数据集由带标签的示例组成，每个示例由特征向量表示，特征向量中的每个维度描述了示例的某个方面。为了提高预测性能，数据分析师需要创造性地设计高信息量的特征。这种特征可以帮助学习算法更好地预测训练数据的标签。比如，在用户行为日志中，可以提取用户的订阅价格、每日连接频率、平均会话时长等作为特征。\n",
    "\n",
    "### 5.1.1 独热编码\n",
    "当数据集包含类别型特征时（例如颜色），需要将其转换为数值型特征。一种常用的方法是将类别型特征进行独热编码，将每个类别转化为一个二进制向量。例如，“颜色”特征包含“红色”“黄色”和“绿色”，则可以将其转换为：\n",
    "- 红色: [1, 0, 0]\n",
    "- 黄色: [0, 1, 0]\n",
    "- 绿色: [0, 0, 1]\n",
    "\n",
    "## 5.2 过拟合\n",
    "过拟合是机器学习模型常见的问题之一，指模型过度拟合训练数据，以至于对新数据的泛化能力很差。为了缓解这一问题，常采用正则化技术。模型有**低偏差**时，能很好地预测训练数据，但容易出现过拟合。\n",
    "\n",
    "## 5.3 超参数调优\n",
    "超参数是在运行算法之前由数据分析师设定的参数，不同于模型的学习参数。超参数调优指的是通过实验找到最佳的超参数组合，提升模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ff421-1108-43b0-bdda-56457233507a",
   "metadata": {},
   "source": [
    "# 第6章 神经网络和深度学习\n",
    "\n",
    "## 6.1 神经网络\n",
    "神经网络可以看作是一个数学函数，表示为 \\( y = f_{NN}(x) \\)，该函数是嵌套的。神经网络包含多个层次，每一层的输出成为下一层的输入。例如，对于三层神经网络，其表达式为：\n",
    "\\[ y = f_{NN}(x) = f_3(f_2(f_1(x))) \\]\n",
    "\n",
    "每层由激活函数 \\( g \\) 和参数矩阵 \\( W \\) 及偏置向量 \\( b \\) 组成，激活函数通常是非线性的，帮助神经网络进行复杂的非线性映射。\n",
    "\n",
    "### 6.1.1 多层感知机（MLP）\n",
    "MLP 是一种前馈神经网络 (Feed-Forward Neural Network, FFNN)，典型结构包含输入层、多个隐藏层和输出层。在每一层，神经元对输入向量进行线性变换，并通过激活函数产生输出。MLP 通常用于回归和分类任务。\n",
    "\n",
    "## 6.2 深度学习\n",
    "深度学习指的是具有多于两层隐藏层的神经网络的训练。随着层数的增加，神经网络面临“梯度消失”和“梯度爆炸”问题，这在使用梯度下降算法时会阻碍训练。然而，现代的优化方法（如ReLU激活函数和批量归一化）已经大大减轻了这些问题，使得非常深的神经网络得以成功训练。\n",
    "\n",
    "### 6.2.1 卷积神经网络 (CNN)\n",
    "CNN 是一种特别适合图像和文本处理的前馈神经网络。其结构通过卷积层显著减少参数的数量，而不损失模型的表现质量。卷积层使用滤波器对图像进行局部特征提取，每个滤波器识别图像中的不同模式（如边缘、纹理等）。\n",
    "\n",
    "### 6.2.2 循环神经网络 (RNN)\n",
    "RNN 主要用于处理序列数据，如文本和语音。与前馈网络不同，RNN 的神经元具有状态，可以记住之前的输入信息。每个时间步的输入不仅包括当前的输入向量，还包括上一时间步的隐藏状态，使得网络能够捕捉序列中的时间依赖性。\n",
    "\n",
    "## 6.3 训练神经网络中的挑战\n",
    "训练神经网络面临的一个主要问题是数据的表示形式。对于图像，首先需要调整尺寸，然后进行标准化和归一化；对于文本，需要进行标记化处理，将词语转化为向量表示。此外，随着网络的复杂性增加，训练时间和计算资源需求也随之上升。通过数据增强、正则化（如 dropout）和其他优化技巧，可以改善模型的泛化性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f124c67-28b1-479e-aa8e-47d678934809",
   "metadata": {},
   "source": [
    "# 第7章 问题和方案\n",
    "\n",
    "## 7.1 核回归\n",
    "核回归是一种非参数方法，用于处理数据不呈线性关系的情况。它通过核函数（如高斯核）来计算输入点和样本点的相似度，从而预测目标值。核回归的权重随着输入点和样本点的相似性增大而增大，通过核函数确定权重分布。模型的平滑度通过超参数b进行调整。\n",
    "\n",
    "## 7.2 多类分类\n",
    "在多类分类问题中，标签可以是多个类别之一。虽然某些机器学习算法（如决策树）可以自然扩展为多类分类，但二元分类算法如SVM需要转化为多分类。常见方法包括“一对其余”，将多类问题转为多个二元分类问题，每个分类器处理一个类别与其他类别的区分。\n",
    "\n",
    "## 7.3 单类分类\n",
    "单类分类旨在从仅有一个类别的训练集中学习出该类别与其他类别的差异，用于异常检测和新奇检测。常见方法有一类高斯、一类SVM等，这些方法通过模型化已知类别的数据分布来检测异常样本。\n",
    "\n",
    "## 7.4 多标签分类\n",
    "多标签分类允许一个样本同时具有多个标签。在这种情况下，算法需输出多个标签而非一个单独的标签。通过一对其余策略或决策树、神经网络等算法，可以实现多标签分类任务。\n",
    "\n",
    "## 7.5 集成学习\n",
    "集成学习通过组合多个模型来提高单一模型的性能。常见方法包括Boosting和Bagging。随机森林和梯度提升是两种重要的集成学习方法，前者通过多个决策树构成，后者通过逐步修正模型的预测误差。\n",
    "\n",
    "## 7.6 序列标注学习\n",
    "序列标注学习解决如何为输入序列（如文本）分配标签序列的问题。常见应用包括词性标注、命名实体识别等。序列到序列学习（如用于机器翻译的模型）通过编码器-解码器结构来处理输入输出序列。\n",
    "\n",
    "## 7.7 主动学习\n",
    "在主动学习中，算法主动选择最有信息的样本进行标注，从而在较少标注数据下训练出性能优异的模型。这种方法特别适用于标注成本高昂的场景。\n",
    "\n",
    "## 7.8 半监督学习\n",
    "半监督学习结合了少量有标签数据和大量无标签数据来构建模型。通过使用无标签数据来改善模型的泛化能力，这种方法特别适合标签数据难以获取的场景。\n",
    "\n",
    "## 7.9 一次性学习\n",
    "一次性学习常用于人脸识别任务，旨在通过仅一个样本实现识别。常用方法包括孪生神经网络，它通过比较两个输入的嵌入向量来判断两张图片是否属于同一对象。\n",
    "\n",
    "## 7.10 零样本学习\n",
    "零样本学习的目标是预测未在训练集中出现过的类别标签。通过利用词嵌入等技术，模型可以根据训练集中未见过的类别特征进行推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ddeb4-2be6-43f5-8a7f-9cb882de508a",
   "metadata": {},
   "source": [
    "# 第8章 高级实践\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e25995-c84b-42bc-91a6-6bd32dd7e0ab",
   "metadata": {},
   "source": [
    "# 第9章 无监督学习\r\n",
    "\r",
    "### 9.1 密度估计\r\n",
    "密度估计是通过数据来建模概率密度函数（pdf）。在机器学习中，密度估计可用于检测异常和入侵等问题。模型可以是参数化的，例如多元正态分布，也可以使用非参数化模型，如核密度估计。通过选定合适的核函数和带宽，可以生成目标密度的估计值。\r",
    "\r\n",
    "### 9.2 聚类\r\n",
    "聚类是一种无监督学习方法，用于从未标注数据中学习为样本分配标签的模型。常见的聚类算法包括K均值算法、DBSCAN和HDBSCAN。\r\n",
    "\r\n",
    "- **K-Means**：通过随机初始化质心，计算每个样本与质心的距离，分配样本到最近的质心。算法重复质心更新和样本重新分配过程，直到分配不再变化。\r\n",
    "- **DBSCAN和HDBSCAN**：DBSCAN基于密度的聚类方法，能够处理形状复杂的簇。相比于K均值，DBSCAN不需要预先定义簇的数量，但需要定义两个超参数：ε和最小点数n。HDBSCAN是DBSCAN的改进版本，移除了ε的需求，并能够处理密度不匀的簇。\r\n",
    "\r\n",
    "### 9.3 降维\r\n",
    "降维方法在数据可视化和提高模型可解释性方面有重要作用，常用的降维技术包括：\r\n",
    "- **主成分分析（PCA）**：通过找出数据中方差最大的方向，创建新的坐标轴，用于降低数据的维度。\r\n",
    "- **UMAP和自编码器**：UMAP是一种非线性降维技术，自编码器是一种通过神经网络学习低维表示的模型。\r\n",
    "\r\n",
    "这三大部分为无监督学习提供了重要工具，适用于各种数据建模和分析任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d407ff0-56ba-46e7-b5f0-ead9e4d02499",
   "metadata": {},
   "source": [
    "# 第10章 其他形式的学习\n",
    "第十章概述了几种不同于传统监督学习和无监督学习的机器学习方法。以下是按章节目录对内容的总结：\r\n",
    "\r\n",
    "1. **距离度量学习（Metric Learning）**：\r\n",
    "    - 传统的相似性度量如欧几里得距离和余弦相似性虽然常用，但并不总是适合所有数据集。距离度量学习的目标是从数据中学习适合特定任务的度量函数。这些度量可以应用于需要计算距离的算法，如K-means和KNN。通过引入参数矩阵，距离函数可以通过梯度下降法来优化，使其适应不同的数据集【6:2†source】。\r\n",
    "\r\n",
    "2. **排序学习（Learning to Rank）**：\r\n",
    "    - 排序学习是为搜索引擎优化等任务设计的一种有监督学习问题。它的目标是根据文档的特征输出一个排序函数，使得文档的排序与理想的顺序尽可能接近。常见的解决方法包括点对排序、成对排序以及列表排序【6:4†source】。\r\n",
    "\r\n",
    "3. **自监督学习（Self-Supervised Learning）**：\r\n",
    "    - 自监督学习是通过无标签的数据生成监督信号以训练模型。第十章以词嵌入为例，讲解了如何通过上下文预测词汇，从而学习表示单词的特征向量。一个常见的方法是word2vec中的skip-gram模型，它利用大量文本数据通过预测目标词的上下文来学习词汇的嵌入表示【6:3†source】【6:16†source】。\r\n",
    "\r\n",
    "这些学习方法为不同场景中的模型优化提供了更多灵活性和可能性，尤其是在需要定制距离度量或对排序任务有要求的情况下。这些方法在实际应用中可以结合传统的机器学习模型，提升其性能和适应性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34c711-034b-4246-94b5-591f4cdf473a",
   "metadata": {},
   "source": [
    "# 第11章 总结\r\n",
    "\r\n",
    "1. **主题建模 (Topic Modeling)**  \r\n",
    "主题建模是无监督学习中的常见问题，旨在从文本集合中发现主题。Latent Dirichlet Allocation (LDA) 是一种有效的主题发现算法，用于识别文本中的主题并为每个单词分配一个主题。\r\n",
    "\r\n",
    "2. **高斯过程 (Gaussian Processes)**  \r\n",
    "高斯过程是监督学习方法，可用于回归任务。其优势在于提供回归线在每个点的置信区间。\r\n",
    "\r\n",
    "3. **广义线性模型 (Generalized Linear Models, GLM)**  \r\n",
    "GLM 是线性回归的推广模型，能够处理不同形式的输入与目标变量之间的关系。Logistic回归就是GLM的一种形式。\r\n",
    "\r\n",
    "4. **概率图模型 (Probabilistic Graphical Models, PGM)**  \r\n",
    "PGM 通过图结构描述变量之间的条件依赖关系。每个节点表示一个随机变量，边表示随机变量之间的条件依赖。常见的 PGM 模型包括贝叶斯网络和条件随机场。\r\n",
    "\r\n",
    "5. **马尔可夫链蒙特卡洛方法 (Markov Chain Monte Carlo, MCMC)**  \r\n",
    "MCMC 是从复杂概率分布中采样的算法，用于解决概率图模型中的采样问题。\r\n",
    "\r\n",
    "6. **生成对抗网络 (Generative Adversarial Networks, GANs)**  \r\n",
    "GAN 是一类用于无监督学习的神经网络，通过两个神经网络的对抗来生成真实感极强的图像。一个生成网络用于生成图像，另一个判别网络用于区分生成的图像与真实图像。\r\n",
    "\r\n",
    "7. **遗传算法 (Genetic Algorithms)**  \r\n",
    "遗传算法是一种用于优化不可微目标函数的数值优化技术。它模拟生物进化过程，通过选择、交叉和变异等操作寻找全局最优解。\r\n",
    "\r\n",
    "8. **强化学习 (Reinforcement Learning)**  \r\n",
    "强化学习处理的是序列决策问题，目标是优化长期奖励。Q-learning 是一种常见的强化学习算法，广泛应用于视频游戏、机器人导航、供应链管理等领域【6:0†The Hundred Page Machine Learning Book】。\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
